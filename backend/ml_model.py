import torch
import os
from transformers import AutoTokenizer, AutoModelForSequenceClassification, BitsAndBytesConfig
from peft import PeftModel
from config import settings

class SentimentModel:
    def __init__(self):
        print("ðŸš€ Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð¼Ð¾Ð´ÐµÐ»Ð¸...")
        
        # 1. Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ Ñ‚Ð¾ÐºÐµÐ½Ð¸Ð·Ð°Ñ‚Ð¾Ñ€
        self.tokenizer = AutoTokenizer.from_pretrained(settings.base_model_name)

        # 2. ÐšÐ¾Ð½Ñ„Ð¸Ð³ Ð´Ð»Ñ ÑÐºÐ¾Ð½Ð¾Ð¼Ð¸Ð¸ Ð¿Ð°Ð¼ÑÑ‚Ð¸ (8-bit)
        bnb_config = BitsAndBytesConfig(
            load_in_8bit=True,
            llm_int8_threshold=6.0
        )

        # 3. Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ Ð‘ÐÐ—ÐžÐ’Ð£Ð® Ð¼Ð¾Ð´ÐµÐ»ÑŒ (ÑÐºÐµÐ»ÐµÑ‚)
        print(f"ðŸ“¦ Loading Base Model: {settings.base_model_name}...")
        self.base_model = AutoModelForSequenceClassification.from_pretrained(
            settings.base_model_name,
            num_labels=3,
            quantization_config=bnb_config,
            device_map="auto"
        )

        # 4. Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐµÐ¼ PEFT Ñ Ð”Ð•Ð¤ÐžÐ›Ð¢ÐÐ«Ðœ Ð°Ð´Ð°Ð¿Ñ‚ÐµÑ€Ð¾Ð¼
        # ÐœÑ‹ Ð´Ð°ÐµÐ¼ ÐµÐ¼Ñƒ Ð¸Ð¼Ñ "default", Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð¿Ð¾Ñ‚Ð¾Ð¼ Ð»ÐµÐ³ÐºÐ¾ Ðº Ð½ÐµÐ¼Ñƒ Ð²Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°Ñ‚ÑŒÑÑ
        print(f"ðŸ”Œ Loading Default Adapter: {settings.adapter_name}...")
        try:
            self.model = PeftModel.from_pretrained(
                self.base_model,
                settings.adapter_name,
                adapter_name="default", 
                is_trainable=False
            )
            print("âœ… Default adapter loaded and active.")
        except Exception as e:
            print(f"âŒ Error loading default adapter: {e}")
            # Ð•ÑÐ»Ð¸ Ð½Ðµ Ð²Ñ‹ÑˆÐ»Ð¾, Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð¾ÑÑ‚Ð°ÐµÑ‚ÑÑ Ð¿Ñ€Ð¾ÑÑ‚Ð¾ Ð¾Ð±ÐµÑ€Ñ‚ÐºÐ¾Ð¹ Ð½Ð°Ð´ Ð±Ð°Ð·Ð¾Ð¹
            self.model = self.base_model

        self.active_adapter_name = "default"

    def switch_model(self, model_name: str):
        """
        ÐŸÐµÑ€ÐµÐºÐ»ÑŽÑ‡Ð°ÐµÑ‚ Ð°ÐºÑ‚Ð¸Ð²Ð½Ñ‹Ð¹ Ð°Ð´Ð°Ð¿Ñ‚ÐµÑ€, Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÑ set_adapter.
        ÐÐµ Ð¿ÐµÑ€ÐµÐ·Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÑ‚ Ð±Ð°Ð·Ð¾Ð²ÑƒÑŽ Ð¼Ð¾Ð´ÐµÐ»ÑŒ.
        """
        # 1. ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»ÑÐµÐ¼ Ñ†ÐµÐ»ÐµÐ²Ð¾Ðµ Ð¸Ð¼Ñ Ð°Ð´Ð°Ð¿Ñ‚ÐµÑ€Ð°
        # Ð•ÑÐ»Ð¸ Ð¿Ñ€Ð¸ÑˆÐ»Ð¾ None, "Default" Ð¸Ð»Ð¸ Ð¿ÑƒÑÑ‚Ð°Ñ ÑÑ‚Ñ€Ð¾ÐºÐ° -> Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ "default"
        switch_modeltarget_adapter = "default"
        if model_name and model_name not in ["Default", "Base", "default"]:
            target_adapter = model_name

        # 2. Ð•ÑÐ»Ð¸ Ð¼Ñ‹ ÑƒÐ¶Ðµ Ð½Ð° ÑÑ‚Ð¾Ð¼ Ð°Ð´Ð°Ð¿Ñ‚ÐµÑ€Ðµ - Ð²Ñ‹Ñ…Ð¾Ð´Ð¸Ð¼
        if self.active_adapter_name == target_adapter:
            return

        print(f"Ð¿ÐµÑ€ÐµÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ðµ Ð°Ð´Ð°Ð¿Ñ‚ÐµÑ€Ð° Ð½Ð°'{target_adapter}'...")

        # 3. Ð•ÑÐ»Ð¸ Ñ…Ð¾Ñ‚Ð¸Ð¼ Ð²ÐµÑ€Ð½ÑƒÑ‚ÑŒÑÑ Ðº Ð´ÐµÑ„Ð¾Ð»Ñ‚Ð½Ð¾Ð¼Ñƒ
        if target_adapter == "default":
            try:
                self.model.set_adapter("default")
                self.active_adapter_name = "default"
                print("Ð¿ÐµÑ€ÐµÐºÐ»ÑŽÑ‡ÐµÐ½Ð¾ Ð½Ð° Ð´ÐµÑ„Ð¾Ð»Ñ‚Ð½Ñ‹Ð¹ Ð°Ð´Ð°Ð¿Ñ‚ÐµÑ€.")
            except Exception as e:
                print(f"Ð½Ðµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð¿ÐµÑ€ÐµÐºÐ»ÑŽÑ‡Ð¸Ñ‚ÑŒ Ð½Ð° Ð´ÐµÑ„Ð¾Ð»Ñ‚Ð½Ñ‹Ð¹ Ð°Ð´Ð°Ð¿Ñ‚ÐµÑ€: {e}")
            return

        # 4. Ð•ÑÐ»Ð¸ ÑÑ‚Ð¾ ÐºÐ°ÑÑ‚Ð¾Ð¼Ð½Ð°Ñ Ð¼Ð¾Ð´ÐµÐ»ÑŒ
        # Ð¡Ð½Ð°Ñ‡Ð°Ð»Ð° Ð¿Ñ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼, Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð° Ð»Ð¸ Ð¾Ð½Ð° ÑƒÐ¶Ðµ Ð² Ð¿Ð°Ð¼ÑÑ‚ÑŒ
        if target_adapter in self.model.peft_config:
            self.model.set_adapter(target_adapter)
            self.active_adapter_name = target_adapter
            print(f"Ð¿ÐµÑ€ÐµÐºÐ»ÑŽÑ‡ÐµÐ½Ð¾ Ð½Ð° ÐºÐµÑˆÐ¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ð¹ Ð°Ð´Ð°Ð¿Ñ‚ÐµÑ€: {target_adapter}")
        else:
            # Ð•ÑÐ»Ð¸ Ð² Ð¿Ð°Ð¼ÑÑ‚Ð¸ Ð½ÐµÑ‚, Ð¿Ñ€Ð¾Ð±ÑƒÐµÐ¼ Ð·Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚ÑŒ Ñ Ð´Ð¸ÑÐºÐ°
            adapter_path = f"./trained_models/{target_adapter}"
            if not os.path.exists(adapter_path):
                print(f" ÐŸÑƒÑ‚ÑŒ Ð°Ð´Ð°Ð¿Ñ‚ÐµÑ€Ð° Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½: {adapter_path}. ÐžÑÑ‚Ð°ÐµÑ‚ÑÑ Ð½Ð° Ñ‚ÐµÐºÑƒÑ‰ÐµÐ¼.")
                return

            try:
                print(f"ðŸ“‚ Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð½Ð¾Ð²Ð¾Ð¹ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð¸Ð· Ð´Ð¸ÑÐºÐ°: {target_adapter}")
                self.model.load_adapter(adapter_path, adapter_name=target_adapter)
                self.model.set_adapter(target_adapter)
                self.active_adapter_name = target_adapter
                print(f"âœ… Ð—Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð° Ð¸ Ð¿ÐµÑ€ÐµÐºÐ»ÑŽÑ‡ÐµÐ½Ð° Ð½Ð°: {target_adapter}")
            except Exception as e:
                print(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸ Ð°Ð´Ð°Ð¿Ñ‚ÐµÑ€Ð° {target_adapter}: {e}")
                # Ð•ÑÐ»Ð¸ Ð¾ÑˆÐ¸Ð±ÐºÐ°, Ð¿Ñ‹Ñ‚Ð°ÐµÐ¼ÑÑ Ð²ÐµÑ€Ð½ÑƒÑ‚ÑŒÑÑ Ð½Ð° Ð´ÐµÑ„Ð¾Ð»Ñ‚
                self.model.set_adapter("default")
                self.active_adapter_name = "default"

    def predict(self, text: str, model_name: str = None) -> dict:
        # Ð¡Ð½Ð°Ñ‡Ð°Ð»Ð° Ð¿ÐµÑ€ÐµÐºÐ»ÑŽÑ‡Ð°ÐµÐ¼ Ð°Ð´Ð°Ð¿Ñ‚ÐµÑ€
        self.switch_model(model_name)

        if not text:
            return {"label": "neutral", "score": 0.0}

        # Ð¢Ð¾ÐºÐµÐ½Ð¸Ð·Ð°Ñ†Ð¸Ñ
        inputs = self.tokenizer(
            text,
            return_tensors="pt",
            truncation=True,
            padding=True,
            max_length=256
        )
        
        # ÐŸÐµÑ€ÐµÐ½Ð¾Ñ Ð½Ð° GPU
        inputs = {k: v.to(self.model.device) for k, v in inputs.items()}

        with torch.no_grad():
            outputs = self.model(**inputs)
            probs = torch.softmax(outputs.logits, dim=1)

        score, label_id = torch.max(probs, dim=1)

        return {
            "label": f"LABEL_{label_id.item()}",
            "score": float(score.item())
        }

# Singleton
sentiment_model = None

def get_sentiment_model() -> SentimentModel:
    global sentiment_model
    if sentiment_model is None:
        sentiment_model = SentimentModel()
    return sentiment_model
